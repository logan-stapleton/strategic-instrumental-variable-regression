{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8r6SJk9X5Si"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue-tz44NOFZB"
   },
   "outputs": [],
   "source": [
    "### First, estimate theta*, true causal effect of (SAT, HS GPA) on college GPA\n",
    "## based on real data from 1000 students\n",
    "df = pd.read_csv(\"clean_gpa.csv\")\n",
    "\n",
    "x_real = df[['new_sat','new_gpa']].to_numpy()\n",
    "y_real = df['college_gpa'].to_numpy()\n",
    "\n",
    "## find true causal effects theta*\n",
    "# ordinary least squares (ols)\n",
    "x_tilde = np.hstack((x_real,np.ones((len(x_real),1)))) # add 1 for intercept\n",
    "\n",
    "m = x_real.shape[1]\n",
    "x_sum = np.zeros([m+1,m+1])\n",
    "xy_sum = np.zeros(m+1)\n",
    "\n",
    "for i in range(len(x_real)):\n",
    "  x_sum += np.outer(x_tilde[i],x_tilde[i])\n",
    "  xy_sum += x_tilde[i]*y_real[i]\n",
    "\n",
    "theta_star = np.matmul(np.linalg.inv(x_sum),xy_sum)[:-1]\n",
    "\n",
    "# set theta* to nice values for synthetic data\n",
    "# 1st entry in theta* is for SAT score, 2nd is for HSGPA\n",
    "theta_star = np.array([0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liotSvfrhzUz"
   },
   "outputs": [],
   "source": [
    "def test_params(num_applicants=1000, EW = np.matrix([[10.0,0],[0,1.0]]), theta_star=theta_star):\n",
    "  # inputs:  num_applicants = number of applicants (time horizon T), \n",
    "  #          EW = expected effort conversion matrix E[W],\n",
    "  #          theta_star = true causal effects theta* (set to [0,0.5] by default)\n",
    "  #\n",
    "  # output:  synthetic data for num_applicants rounds, including:\n",
    "  #           z (unobserved, unmanipulated features), \n",
    "  #           x (observed, manipulated features), y (outcome), \n",
    "  #           theta (decision rule), and WWT (effort conversion matrix)\n",
    "  #           \n",
    "  #           estimate_list: OLS & 2SLS estimates @ rounds 10 to num_applicants\n",
    "  #           error_list: L2-norm of OLS & 2SLS estimates minus true theta*\n",
    "  #\n",
    "  # outline: 1) create synthetic unobserved data (z_t, W_tW_t^T, g_t), \n",
    "  #             add confounding by splitting data into two types split 50/50,\n",
    "  #             (1st half disadvantaged, 2nd half advantaged), \n",
    "  #             make z & WW^T worse for disadvantaged, better for advantaged\n",
    "  #             & set mean g lesser for disadvantaged, high for advantaged\n",
    "  #          2) set decision rule theta_t & solve for x_t and y_t based on model\n",
    "  #          3) OLS estimate by regressing x onto y (w/ intercept estimate)\n",
    "  #          4) 2SLS estimate by regressing x onto theta, then theta onto y (w/ intercept estimate)\n",
    "\n",
    "  num_applicants = num_applicants\n",
    "  half = int(num_applicants/2) \n",
    "  m = theta_star.size\n",
    "\n",
    "  sigma_g = 0.1 # g variance term\n",
    "  mean_sat = 900\n",
    "  mean_gpa = 2\n",
    "  sigma_sat = 200\n",
    "  sigma_gpa = 0.5\n",
    "\n",
    "  # initial features (z)\n",
    "  z = np.zeros([num_applicants,m])\n",
    "\n",
    "  # disadvantaged students\n",
    "  z[0:half,0] = np.random.normal(mean_sat-100,sigma_sat,z[0:half,0].shape) #SAT\n",
    "  z[0:half,1] = np.random.normal(mean_gpa-.2,sigma_gpa,z[0:half,1].shape) #GPA\n",
    "\n",
    "  # advantaged students\n",
    "  z[half:,0] = np.random.normal(mean_sat+100,sigma_sat,z[0:half,0].shape) #SAT\n",
    "  z[half:,1] = z[half:,1] + np.random.normal(mean_gpa+.2,sigma_gpa,z[half:,1].shape) #GPA\n",
    "\n",
    "  z[:,0] = np.clip(z[:,0],400,1600) # clip to 400 to 1600\n",
    "  z[:,1] = np.clip(z[:,1],0,4) # clip to 0 to 4.0\n",
    "\n",
    "  # confounding error term g (error on true college GPA)\n",
    "  g = np.ones(num_applicants)*0.5 # legacy students shifted up\n",
    "  g[0:half]=-0.5 # first-gen students shifted down\n",
    "  g += np.random.normal(1,0.2,size=num_applicants) # non-zero-mean\n",
    "\n",
    "  # assessment rule \n",
    "  \n",
    "  #theta = np.zeros([num_applicants,z.shape[1]])\n",
    "  #theta = np.random.normal(1,1,[num_applicants,z.shape[1]])\n",
    "  #theta[:,0]*=7.5 # scaling for SAT score\n",
    "  theta = np.random.multivariate_normal([1,1],[[10, 0], [0, 1]],num_applicants)\n",
    "\n",
    "  # effort conversion matrices W_t*W_t^T\n",
    "  WWT = list()\n",
    "\n",
    "  EW = EW\n",
    "\n",
    "  for i in range(num_applicants):\n",
    "    W_t = EW.copy()\n",
    "\n",
    "    # add noise\n",
    "    noise_mean = [0.5, 0, 0, 0.1]\n",
    "    noise_cov = [[0.25, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0.01]]\n",
    "\n",
    "    noise = np.random.multivariate_normal(noise_mean, noise_cov).reshape(W_t.shape)\n",
    "\n",
    "    if i<half: # disadvantaged\n",
    "      W_t -= noise\n",
    "    else: # advantaged\n",
    "      W_t += noise\n",
    "\n",
    "    WWT.append(np.matmul(W_t,W_t.T))\n",
    "    #if i % 100 == 0:\n",
    "    #  print(np.matmul(W_t,W_t.T))\n",
    "  WWT = np.array(WWT)\n",
    "\n",
    "  # observable features x\n",
    "  x = np.zeros([num_applicants,z.shape[1]])\n",
    "  for i in range(num_applicants):\n",
    "    x[i] = z[i] + np.matmul(WWT[i],theta[i])\n",
    "  \n",
    "  x[:,0] = np.clip(x[:,0],400,1600) # clip to 400 to 1600\n",
    "  x[:,1] = np.clip(x[:,1],0,4) # clip to 0 to 4.0\n",
    "\n",
    "  # true outcomes (college gpa)\n",
    "  y = np.clip(np.matmul(x,theta_star) + g,0,4) # clipped outcomes\n",
    "  #y = np.matmul(x,theta_star) + g # not clipped outcomes\n",
    "\n",
    "  '''def ols(x,y,T): # regular\n",
    "    # estimate theta*: regress x onto y (with centering)\n",
    "    m = x.shape[1]\n",
    "    x_sum = np.zeros([m,m])\n",
    "    xy_sum = np.zeros(m)\n",
    "\n",
    "    for i in range(T):\n",
    "      x_sum += np.outer(x[i],x[i])\n",
    "      xy_sum += x[i]*y[i]\n",
    "\n",
    "    return np.matmul(np.linalg.inv(x_sum),xy_sum)'''\n",
    "\n",
    "  def ols(x,y,T): # with intercept estimation\n",
    "    x_tilde = np.hstack((x,np.ones((len(x),1)))) # for parameter estimation\n",
    "\n",
    "    m = x.shape[1]\n",
    "    x_sum = np.zeros([m+1,m+1])\n",
    "    xy_sum = np.zeros(m+1)\n",
    "\n",
    "    for i in range(T):\n",
    "      x_sum += np.outer(x_tilde[i],x_tilde[i])\n",
    "      xy_sum += x_tilde[i]*y[i]\n",
    "\n",
    "    theta_hat_ols = np.matmul(np.linalg.inv(x_sum),xy_sum)\n",
    "    return theta_hat_ols[:m]\n",
    "    \n",
    "  def tsls(x,y,theta,T): # runs until round T\n",
    "    theta_tilde = np.hstack((theta,np.ones((len(theta),1)))) # for parameter estimation\n",
    "\n",
    "    m = x.shape[1]\n",
    "    theta_tilde_sum = np.zeros([m+1,m+1])\n",
    "    xtheta_tilde_sum = np.zeros([m+1,m])\n",
    "    ytheta_tilde_sum = np.zeros(m+1)\n",
    "\n",
    "    for i in range(T):\n",
    "      theta_tilde_sum += np.outer(theta_tilde[i],theta_tilde[i])\n",
    "      xtheta_tilde_sum += np.outer(theta_tilde[i],x[i])\n",
    "      ytheta_tilde_sum += theta_tilde[i]*y[i]\n",
    "\n",
    "    # Step 1) estimate Omega: regress theta onto x\n",
    "    omega_hat = np.matmul(np.linalg.inv(theta_tilde_sum),xtheta_tilde_sum)\n",
    "    z_bar = omega_hat[m,:]\n",
    "    omega_hat = omega_hat[:m,:m] \n",
    "\n",
    "    # Step 2) estimate Lambda: regress theta onto y\n",
    "    lambda_hat = np.matmul(np.linalg.inv(theta_tilde_sum),ytheta_tilde_sum)\n",
    "    gztheta_bar = lambda_hat[m]\n",
    "    lambda_hat = lambda_hat[:m]\n",
    "\n",
    "    # Step 3) estimate theta*: inverse(Omega-hat)*Lambda-hat\n",
    "    theta_hat_tsls = np.matmul(np.linalg.inv(omega_hat),lambda_hat)\n",
    "    return theta_hat_tsls\n",
    "\n",
    "  # save estimates and errors for every even round \n",
    "  estimates_list = np.zeros([int((num_applicants/2)),2,2])\n",
    "  error_list = np.zeros([int((num_applicants)/2),2])\n",
    "  i=0\n",
    "\n",
    "  # shuffle the samples so types show up randomly\n",
    "  [x_shuffle,y_shuffle,theta_shuffle] = [x.copy(),y.copy(),theta.copy()]\n",
    "  shuffle_iter = list(range(len(x)))\n",
    "  np.random.shuffle(shuffle_iter)\n",
    "\n",
    "  j = 0\n",
    "  for k in shuffle_iter:\n",
    "    x_shuffle[j] = x[k]\n",
    "    y_shuffle[j] = y[k]\n",
    "    theta_shuffle[j] = theta[k]\n",
    "    j+=1\n",
    "\n",
    "  for t in range(10,num_applicants,2):\n",
    "    # centering\n",
    "    #y_mean = np.mean(y_shuffle[:t])\n",
    "    #x_mean = np.mean(x_shuffle[:t],axis=0)\n",
    "\n",
    "    # estimates\n",
    "    ols_estimate = ols(x_shuffle, y_shuffle, t) # ols w/ intercept estimate\n",
    "    tsls_estimate = tsls(x_shuffle, y_shuffle, theta_shuffle, t) # 2sls w/ intercept estimate\n",
    "    estimates_list[i,:] += [ols_estimate,tsls_estimate]\n",
    "\n",
    "    # errors\n",
    "    ols_error = np.linalg.norm(theta_star-ols_estimate)\n",
    "    tsls_error = np.linalg.norm(theta_star-tsls_estimate)\n",
    "    error_list[i] = [ols_error,tsls_error]\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "  return [estimates_list, error_list, y, x, z, theta, WWT, EW, theta_star]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9C8xAAcjazH"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# set T > 10 & in units of 5s for nice plots later \n",
    "T = 510\n",
    "epochs = 1\n",
    "half = int(T/2)\n",
    "\n",
    "estimates_list_mean = np.zeros((epochs,half,2,2))\n",
    "error_list_mean = np.zeros((epochs,half,2))\n",
    "\n",
    "#estimates_list_mean = list()\n",
    "#error_list_mean = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "  np.random.seed(i)\n",
    "  [estimates_list, error_list, y, x, z, theta, WWT, EW, theta_star] = test_params(num_applicants=T)\n",
    "  estimates_list_mean[i,:] = estimates_list\n",
    "  error_list_mean[i,:] = error_list\n",
    "\n",
    "  #estimates_list_mean.append(estimates_list)\n",
    "  #error_list_mean.append(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "dx0ycSFfxKc9",
    "outputId": "fbd2705f-d006-4b26-b2d8-dd5761562523"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "## plot first-gen & legacy shift unobservable features (z) to observable (x) \n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(12,10)) #constrained_layout=False\n",
    "\n",
    "### first-gen HS GPA\n",
    "ax1.hist(x[0:half,1],bins='auto', label='after manipulation', color='darkorange')\n",
    "\n",
    "#ax1.axvline(x=np.mean(x[0:half,1]), color='red', linestyle='--', label='mean after manipulation') # before mean\n",
    "ax1.axvline(x=np.mean(x[0:half,1]), linestyle='-', color = 'red', linewidth = 4) # after mean\n",
    "ax1.axvline(x=np.mean(x[0:half,1]), linestyle='--', color = 'white', linewidth = 4) # after mean\n",
    "\n",
    "ax1.set_title(\"observable high school GPA (x1)\")\n",
    "ax1.set(ylabel='Number of applicants')\n",
    "\n",
    "ax1.hist(z[0:half,1], bins='auto', label='before manipulation', color='yellow', alpha=0.75)\n",
    "#ax1.axvline(x=np.mean(z[0:half,1]), color='blue', linestyle='--', label='mean before manipulation') # before manipulation\n",
    "ax1.axvline(x=np.mean(z[0:half,1]), linestyle='-', color = 'blue', linewidth = 4) # before mean\n",
    "ax1.axvline(x=np.mean(z[0:half,1]), linestyle='--', color = 'white', linewidth = 4) # before mean\n",
    "\n",
    "ax1.set_title(\"Disadvantaged HS GPA before & after manipulation\", fontsize=14)\n",
    "#ax1.set_xlim(0,4) ###CHANGE BACK\n",
    "ax1.set_xlabel('High school GPA (4.0 scale)',fontsize=14)\n",
    "ax1.set_ylabel('Number of applicants',fontsize=14)\n",
    "ax1.tick_params(axis=\"x\", labelsize=14)\n",
    "ax1.tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "#ax1.legend()\n",
    "\n",
    "### 2) first-gen SAT\n",
    "ax2.hist(x[0:half,0], bins='auto', label='after manipulation', color='orange')\n",
    "ax2.axvline(x=np.mean(x[0:half,0]),color='blue',)\n",
    "\n",
    "#ax2.axvline(x=np.mean(x[0:half,0]), color='red', linestyle='--', label='mean after manipulation') # after mean\n",
    "ax2.axvline(x=np.mean(x[0:half,0]), linestyle='-', color = 'red', linewidth = 4) # after mean\n",
    "ax2.axvline(x=np.mean(x[0:half,0]), linestyle='--', color = 'white', linewidth = 4) # after mean\n",
    "\n",
    "ax2.set(xlabel='GPA (4.0 scale)', ylabel='Number of applicants')\n",
    "\n",
    "ax2.hist(z[0:half,0], bins='auto', label='before manipulation', color='yellow', alpha=0.75)\n",
    "\n",
    "#ax2.axvline(x=np.mean(z[0:half,0]), color='blue', linestyle='--', label='mean before manipulation') # before mean\n",
    "ax2.axvline(x=np.mean(z[0:half,0]), linestyle='-', color = 'blue', linewidth = 4) # before mean\n",
    "ax2.axvline(x=np.mean(z[0:half,0]), linestyle='--', color = 'white', linewidth = 4) # before mean\n",
    "\n",
    "ax2.set_title(\"Disadvantaged SAT before & after manipulation\", fontsize=14)\n",
    "#ax2.set_xlim(400,1600) ###CHANGE BACK\n",
    "ax2.set_xlabel('SAT score (400 to 1600 points)',fontsize=14)\n",
    "ax2.set_ylabel('Number of applicants',fontsize=14)\n",
    "ax2.tick_params(axis=\"x\", labelsize=14)\n",
    "ax2.tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "#ax2.legend(loc='upper left', fontsize=14)\n",
    "\n",
    "### 3) non-first-gen HS GPA\n",
    "ax3.hist(x[half:,1],bins='auto', label='after manipulation', color='green')\n",
    "\n",
    "ax3.hist(z[half:,1], bins='auto', label='before manipulation', color='lightgreen', alpha=0.75)\n",
    "\n",
    "#ax3.axvline(x=np.mean(z[half:,1]), color='blue', linestyle='--', label='mean before manipulation') # before mean\n",
    "ax3.axvline(x=np.mean(z[half:,1]), linestyle='-', color = 'blue', linewidth = 4) # before mean\n",
    "ax3.axvline(x=np.mean(z[half:,1]), linestyle='--', color = 'white', linewidth = 4) # before mean\n",
    "\n",
    "#ax3.axvline(x=np.mean(x[half:,1]), color='red', linestyle='--', label='mean after manipulation') # after mean\n",
    "ax3.axvline(x=np.mean(x[half:,1]), linestyle='-', color = 'red', linewidth = 4) # after mean\n",
    "ax3.axvline(x=np.mean(x[half:,1]), linestyle='--', color = 'white', linewidth = 4) # after mean\n",
    "\n",
    "ax3.set_title(\"Advantaged HS GPA before & after manipulation\", fontsize=13)\n",
    "#ax3.set_xlim(0,4) ###CHANGE BACK\n",
    "ax3.set_xlabel('High school GPA (4.0 scale)',fontsize=14)\n",
    "ax3.set_ylabel('Number of applicants',fontsize=14)\n",
    "ax3.tick_params(axis=\"x\", labelsize=14)\n",
    "ax3.tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "\n",
    "### 4) non-first-gen SAT\n",
    "ax4.hist(x[half:,0], bins='auto', label='after manipulation', color='green')\n",
    "\n",
    "ax4.hist(z[half:,0], bins='auto', label='before manipulation', color='lightgreen', alpha=0.75)\n",
    "\n",
    "#ax4.axvline(x=np.mean(z[half:,0]), color='blue', linestyle='--', label='mean before manipulation') # before mean\n",
    "ax4.axvline(x=np.mean(z[half:,0]), linestyle='-', color = 'blue', linewidth = 4) # before mean\n",
    "ax4.axvline(x=np.mean(z[half:,0]), linestyle='--', color = 'white', linewidth = 4) # before mean\n",
    "\n",
    "#ax4.axvline(x=np.mean(x[half:,0]), color='red', linestyle='--', label='mean after manipulation') # before mean\n",
    "ax4.axvline(x=np.mean(x[half:,0]), linestyle='-', color = 'red', linewidth = 4) # before mean\n",
    "ax4.axvline(x=np.mean(x[half:,0]), linestyle='--', color = 'white', linewidth = 4) # before mean\n",
    "\n",
    "ax4.set_title(\"Advantaged SAT before & after manipulation\", fontsize=13)\n",
    "#ax4.set_xlim(400,1600) ###CHANGE BACK\n",
    "ax4.set_xlabel('SAT score (400 to 1600 points)',fontsize=14)\n",
    "ax4.set_ylabel('Number of applicants',fontsize=14)\n",
    "ax4.tick_params(axis=\"x\", labelsize=14)\n",
    "ax4.tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "#ax4.legend(bbox_to_anchor=(-3, 0), loc='upper left', fontsize=14,ncol=4)\n",
    "\n",
    "\n",
    "## legend\n",
    "pre_fg = Patch(color='yellow', label='disadvantaged unobserved (unmanipulated)', alpha=0.75)\n",
    "post_fg = Patch(color='darkorange', label='disadvantaged observed (manipulated)')\n",
    "\n",
    "pre_ls = Patch(color='lightgreen', label='legacy unobserved (unmanipulated)', alpha=0.75)\n",
    "post_ls = Patch(color='green', label='legacy observed (manipulated)')\n",
    "\n",
    "before_mean = Line2D([0], [0], color='blue', linestyle='--', lw=2, label='mean before manipulation')\n",
    "after_mean = Line2D([0], [0], color='red', linestyle='--', lw=2, label='mean after manipulation')\n",
    "\n",
    "fig.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('fg-ls_shifted_features.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "CT3T9FhDjdcj",
    "outputId": "b8b421bf-75b2-40dc-948c-59d0b2183615"
   },
   "outputs": [],
   "source": [
    "# vars for pyplot\n",
    "ticks = list(range(int(T/5), T+1, int(T/5)))\n",
    "ticks.insert(0,1)\n",
    "\n",
    "# plot error of OLS vs 2SLS with error bar\n",
    "plt.errorbar(list(range(2,T+1,2)), np.mean(error_list_mean,axis=0)[:,0], yerr=np.std(error_list_mean,axis=0)[:,0], \n",
    "             color='darkorange', ecolor='wheat', label='OLS',elinewidth=10)\n",
    "plt.errorbar(list(range(2,T+1,2)), np.mean(error_list_mean,axis=0)[:,1], yerr=np.std(error_list_mean,axis=0)[:,1], \n",
    "             color='darkblue', ecolor='lightblue', label='2SLS',elinewidth=10)\n",
    "plt.ylim(0,.25)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim(50,T-11)\n",
    "\n",
    "plt.xlabel('Number of applicants (rounds)', fontsize=14)\n",
    "plt.ylabel('Admissions effect estimate error', fontsize=14)\n",
    "\n",
    "plt.plot(range(1,T+1), 1/np.sqrt(range(1,T+1)), color='red',linestyle='dashed', linewidth=2, label='1/sqrt(T)')\n",
    "plt.plot(range(1,T+1), 1/np.sqrt(range(1,T+1)), linestyle='-', color = 'white', linewidth = 4)\n",
    "plt.plot(range(1,T+1), 1/np.sqrt(range(1,T+1)), linestyle='--', color = 'red', linewidth = 4)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "#plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "#plt.title(\"Estimation error over rounds for OLS vs 2SLS\")\n",
    "\n",
    "plt.savefig('error_estimation.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "G7RWmM8eKG3R",
    "outputId": "3886f8bc-fa32-4b4a-912d-81ee1a881c96"
   },
   "outputs": [],
   "source": [
    "# plot causal effect estimates --OLS vs 2SLS vs theta*\n",
    "temp = 100*estimates_list_mean\n",
    "tsls_mean = np.mean(temp,axis=0)[:,1,0]\n",
    "ols_mean = np.mean(temp,axis=0)[:,0,0]\n",
    "tsls_std = np.std(temp,axis=0)[:,1,0]\n",
    "ols_std = np.std(temp,axis=0)[:,0,0]\n",
    "\n",
    "plt.errorbar(list(range(2,T+1,2)), tsls_mean, yerr=tsls_std, color='darkblue', ecolor='lightblue', label='2SLS effect estimate',elinewidth=10)\n",
    "plt.errorbar(list(range(2,T+1,2)), ols_mean, yerr=ols_std, color='darkorange', ecolor='wheat', label='OLS effect estimate',elinewidth=10)\n",
    "plt.axhline(theta_star[0],label='True effect of HS GPA on college GPA', color='red',linestyle='dashed')\n",
    "plt.axhline(theta_star[0], linestyle='-', color = 'white', linewidth = 4)\n",
    "plt.axhline(theta_star[0], linestyle='--', color = 'red', linewidth = 4)\n",
    "\n",
    "#plt.set_title(\"Estimated SAT effect estimation\")\n",
    "plt.xlabel('Number of applicants (rounds)',fontsize=14)\n",
    "plt.ylabel('Estimated college GPA change \\n per 100 point SAT score increase',fontsize=14)\n",
    "plt.xlim(150,T-11)\n",
    "plt.xticks([],fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylim(-.2,0.4)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.savefig('estimate_convergence.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "m6Yq4N7STSWC",
    "outputId": "0be1fc84-ba6b-454f-85bd-c6e87d0f3238"
   },
   "outputs": [],
   "source": [
    "# plot causal effect estimates --OLS vs 2SLS vs theta*\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,4),constrained_layout=True)\n",
    "ax1.errorbar(list(range(2,T+1,2)), np.mean(estimates_list_mean,axis=0)[:,1,0], yerr=np.std(estimates_list_mean,axis=0)[:,1,0], \n",
    "             color='darkblue', ecolor='lightblue', label='2SLS effect estimate',elinewidth=10)\n",
    "ax1.errorbar(list(range(2,T+1,2)), np.mean(estimates_list_mean,axis=0)[:,0,0], yerr=np.std(estimates_list_mean,axis=0)[:,0,0], \n",
    "             color='darkorange', ecolor='wheat', label='OLS effect estimate',elinewidth=10)\n",
    "ax1.axhline(theta_star[0],label='True effect of HS GPA on college GPA', color='red',linestyle='dashed')\n",
    "ax1.axhline(theta_star[0], linestyle='-', color = 'white', linewidth = 4)\n",
    "ax1.axhline(theta_star[0], linestyle='--', color = 'red', linewidth = 4)\n",
    "\n",
    "#ax1.set_title(\"Estimated SAT effect estimation\")\n",
    "ax1.set_xlabel('Number of applicants (rounds)',fontsize=14)\n",
    "ax1.set_ylabel('Estimated SAT effect on college GPA',fontsize=14)\n",
    "ax1.set_xlim(150,T-11)\n",
    "ax1.tick_params(axis=\"x\", labelsize=14)\n",
    "ax1.tick_params(axis=\"y\", labelsize=14)\n",
    "ax1.set_ylim(-.002,0.004)\n",
    "\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "ax2.errorbar(list(range(2,T+1,2)), np.mean(estimates_list_mean,axis=0)[:,1,1], yerr=np.std(estimates_list_mean,axis=0)[:,1,1], \n",
    "             color='darkblue', ecolor='lightblue', label='2SLS effect estimate',elinewidth=10)\n",
    "ax2.errorbar(list(range(2,T+1,2)), np.mean(estimates_list_mean,axis=0)[:,0,1], yerr=np.std(estimates_list_mean,axis=0)[:,0,1], \n",
    "             color='darkorange', ecolor='wheat', label='OLS effect estimate',elinewidth=10)\n",
    "\n",
    "ax2.axhline(theta_star[1],label='True effect of SAT on college GPA', color='red',linestyle='dashed')\n",
    "ax2.axhline(theta_star[1], linestyle='-', color = 'white', linewidth = 4)\n",
    "ax2.axhline(theta_star[1], linestyle='--', color = 'red', linewidth = 4)\n",
    "\n",
    "#ax2.set_title(\"Estimated HS GPA effect estimation\")\n",
    "ax2.set_xlabel('Number of applicants (rounds)',fontsize=14)\n",
    "ax2.set_ylabel('Estimated HS GPA effect on college GPA',fontsize=12)\n",
    "ax2.tick_params(axis=\"x\", labelsize=14)\n",
    "ax2.tick_params(axis=\"y\", labelsize=14)\n",
    "ax2.set_xlim(150,T-11)\n",
    "ax2.set_ylim(.4,.85)\n",
    "\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "plt.savefig('estimate_convergence.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "knWMuO3qDq4A",
    "outputId": "043c9493-d81a-4b6c-8e60-3effc1ba8b59"
   },
   "outputs": [],
   "source": [
    "#@title  { form-width: \"20%\" }\n",
    "## plot college gpa (outcome y)\n",
    "# combined\n",
    "plt.hist(y,bins='auto',label='combined')\n",
    "plt.axvline(x=np.mean(y),color='blue',linestyle='--', linewidth = 2, label='combined mean')\n",
    "plt.axvline(x=np.mean(y), linestyle='-', color = 'blue', linewidth = 4)\n",
    "plt.axvline(x=np.mean(y), linestyle='--', color = 'white', linewidth = 4)\n",
    "\n",
    "# disadvantaged\n",
    "plt.hist(y[0:half],bins='auto',label='disadvantaged', alpha=.85)\n",
    "plt.axvline(x=np.mean(y[0:half]),color='orange',linestyle='--', linewidth = 2, label='disadvantaged mean')\n",
    "plt.axvline(x=np.mean(y[0:half]), linestyle='-', color = 'orange', linewidth = 4)\n",
    "plt.axvline(x=np.mean(y[0:half]), linestyle='--', color = 'white', linewidth = 4)\n",
    "\n",
    "# advantaged\n",
    "plt.hist(y[half:],bins='auto',label='advantaged', alpha=0.7)\n",
    "plt.axvline(x=np.mean(y[half:]), linestyle='--', color = 'green', linewidth = 2, label='advantaged mean')\n",
    "plt.axvline(x=np.mean(y[half:]),color='green',linestyle='-', linewidth = 4)\n",
    "plt.axvline(x=np.mean(y[half:]), linestyle='--', color = 'white', linewidth = 4)\n",
    "\n",
    "plt.xlim(0,4)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('College GPA (4.0 scale)', fontsize=14)\n",
    "plt.ylabel('Number of applicants', fontsize=14)\n",
    "\n",
    "#plt.title(\"True college GPA (y) for disadvantaged vs. advantaged students\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0, 1.3), loc='upper left', fontsize=12, ncol=2)\n",
    "\n",
    "plt.savefig('all_outcome.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1ilR-yFHK4u"
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "estimates_list.tofile(\"saved_estimates_list\")\n",
    "error_list.tofile(\"saved_error_list\")\n",
    "y.tofile(\"saved_y_list\")\n",
    "x.tofile(\"saved_x_list\")\n",
    "z.tofile(\"saved_z_list\")\n",
    "np.array(WWT).tofile(\"saved_WWT_list\")\n",
    "EWWT.tofile(\"saved_EWWT\")\n",
    "theta_star.tofile(\"saved_theta_star\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOtiaWXesrPC"
   },
   "source": [
    "## Agent outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peSJ6Ae7Ffy0"
   },
   "outputs": [],
   "source": [
    "def ols(x,y,T): # with intercept estimation\n",
    "  x_tilde = np.hstack((x,np.ones((len(x),1)))) # for parameter estimation\n",
    "\n",
    "  m = x.shape[1]\n",
    "  x_sum = np.zeros([m+1,m+1])\n",
    "  xy_sum = np.zeros(m+1)\n",
    "\n",
    "  for i in range(T):\n",
    "    x_sum += np.outer(x_tilde[i],x_tilde[i])\n",
    "    xy_sum += x_tilde[i]*y[i]\n",
    "\n",
    "  theta_hat_ols = np.matmul(np.linalg.inv(x_sum),xy_sum)\n",
    "  return theta_hat_ols[:m]\n",
    "\n",
    "def tsls(x,y,theta,T): # runs until round T\n",
    "  theta_tilde = np.hstack((theta,np.ones((len(theta),1)))) # for parameter estimation\n",
    "\n",
    "  m = x.shape[1]\n",
    "  theta_tilde_sum = np.zeros([m+1,m+1])\n",
    "  xtheta_tilde_sum = np.zeros([m+1,m])\n",
    "  ytheta_tilde_sum = np.zeros(m+1)\n",
    "\n",
    "  for i in range(T):\n",
    "    theta_tilde_sum += np.outer(theta_tilde[i],theta_tilde[i])\n",
    "    xtheta_tilde_sum += np.outer(theta_tilde[i],x[i])\n",
    "    ytheta_tilde_sum += theta_tilde[i]*y[i]\n",
    "\n",
    "  # Step 1) estimate Omega: regress theta onto x\n",
    "  omega_hat = np.matmul(np.linalg.inv(theta_tilde_sum),xtheta_tilde_sum)\n",
    "  z_bar = omega_hat[m,:]\n",
    "  omega_hat = omega_hat[:m,:m] \n",
    "\n",
    "  # Step 2) estimate Lambda: regress theta onto y\n",
    "  lambda_hat = np.matmul(np.linalg.inv(theta_tilde_sum),ytheta_tilde_sum)\n",
    "  gztheta_bar = lambda_hat[m]\n",
    "  lambda_hat = lambda_hat[:m]\n",
    "\n",
    "  # Step 3) estimate theta*: inverse(Omega-hat)*Lambda-hat\n",
    "  theta_hat_tsls = np.matmul(np.linalg.inv(omega_hat),lambda_hat)\n",
    "  return theta_hat_tsls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvVOU-N-Ebuc"
   },
   "outputs": [],
   "source": [
    "num_applicants = 1000\n",
    "half = int(num_applicants/2) \n",
    "\n",
    "theta_star = np.array([0, 0.5])\n",
    "m = theta_star.size\n",
    "\n",
    "sigma_g = 0.1 # g variance term\n",
    "mean_sat = 900\n",
    "mean_gpa = 2\n",
    "sigma_sat = 200\n",
    "sigma_gpa = 0.5\n",
    "\n",
    "# initial features (z)\n",
    "z = np.zeros([num_applicants,m])\n",
    "\n",
    "# disadvantaged students\n",
    "z[0:half,0] = np.random.normal(mean_sat-100,sigma_sat,z[0:half,0].shape) #SAT\n",
    "z[0:half,1] = np.random.normal(mean_gpa-.2,sigma_gpa,z[0:half,1].shape) #GPA\n",
    "\n",
    "# advantaged students\n",
    "z[half:,0] = np.random.normal(mean_sat+100,sigma_sat,z[0:half,0].shape) #SAT\n",
    "z[half:,1] = z[half:,1] + np.random.normal(mean_gpa+.2,sigma_gpa,z[half:,1].shape) #GPA\n",
    "\n",
    "z[:,0] = np.clip(z[:,0],400,1600) # clip to 400 to 1600\n",
    "z[:,1] = np.clip(z[:,1],0,4) # clip to 0 to 4.0\n",
    "\n",
    "# confounding error term g (error on true college GPA)\n",
    "g = np.ones(num_applicants)*0.5 # legacy students shifted up\n",
    "g[0:half]=-0.5 # first-gen students shifted down\n",
    "g += np.random.normal(1,0.2,size=num_applicants) # non-zero-mean\n",
    "\n",
    "# assessment rule \n",
    "theta = np.zeros([num_applicants,z.shape[1]])\n",
    "theta = np.random.normal(1,1,[num_applicants,z.shape[1]])\n",
    "theta[:,0]*=7.5 # scaling for SAT score\n",
    "\n",
    "# expected effort conversion matrices E[WW^T]\n",
    "EWWT = np.matrix([[5,0.05],[0.01,0.4]])\n",
    "\n",
    "# effort conversion matrices W_t*W_t^T\n",
    "WWT = list()\n",
    "\n",
    "for i in range(num_applicants):\n",
    "  WWT_t = EWWT.copy()\n",
    "\n",
    "  # add / subtract noise to E[WW^T]\n",
    "  noise00 = np.random.normal(EWWT[0,0]/10,EWWT[0,0]/10)\n",
    "  noise01 = np.random.normal(EWWT[0,1]/10,EWWT[0,1]/10)\n",
    "  noise10 = np.random.normal(EWWT[1,0]/10,EWWT[1,0]/10)\n",
    "  noise11 = np.random.normal(EWWT[1,1]/10,EWWT[1,1]/10)\n",
    "  noise = np.array([[noise00,noise01],[noise10,noise11]])\n",
    "\n",
    "  if i<half: # first-gen\n",
    "    WWT_t -= noise/2\n",
    "  else: # legacy\n",
    "    noise[0,0]*=7.5\n",
    "    WWT_t += noise*2\n",
    "\n",
    "  WWT.append(WWT_t)\n",
    "WWT = np.array(WWT)\n",
    "\n",
    "# observable features x\n",
    "x = np.zeros([num_applicants,z.shape[1]])\n",
    "for i in range(num_applicants):\n",
    "  x[i] = z[i] + np.matmul(WWT[i],theta[i])\n",
    "\n",
    "x[:,0] = np.clip(x[:,0],400,1600) # clip to 400 to 1600\n",
    "x[:,1] = np.clip(x[:,1],0,4) # clip to 0 to 4.0\n",
    "\n",
    "# true outcomes (college gpa)\n",
    "y = np.clip(np.matmul(x,theta_star) + g,0,4)\n",
    "\n",
    "T = len(x)\n",
    "ols_estimate = ols(x, y, T) # ols w/ intercept estimate\n",
    "tsls_estimate = tsls(x, y, theta, T) # 2sls w/ intercept estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ha_C80uDLJzn",
    "outputId": "02b197ad-52ee-47c2-f2b3-9eb29c4a1fcf"
   },
   "outputs": [],
   "source": [
    "## solve for true Lambda and estimated Lambda\n",
    "# regress theta_t onto y (with centering)\n",
    "m = x.shape[1]\n",
    "theta_sum = np.zeros([m,m])\n",
    "ytheta_sum = np.zeros(m)\n",
    "for i in range(T):\n",
    "  theta_centered = theta[i]-np.mean(theta[:T],axis=0)\n",
    "  theta_sum += np.outer(theta_centered,theta_centered)\n",
    "  ytheta_sum += theta_centered*(y[i]-np.mean(y[:T]))\n",
    "\n",
    "Lambda_hat = np.matmul(np.linalg.inv(theta_sum),ytheta_sum)\n",
    "print(Lambda_hat)\n",
    "\n",
    "Lambda = theta_star*EWWT\n",
    "Lambda = np.array([Lambda[0,0],Lambda[0,1]]) # recast into vector\n",
    "print(Lambda)\n",
    "\n",
    "print(np.linalg.norm(Lambda_hat-Lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4RQIKNsBJ1Z",
    "outputId": "1dfa1b4e-7c0e-4d4b-a47b-7709093a665e"
   },
   "outputs": [],
   "source": [
    "## solve for AO maximizing theta (theta_ao)\n",
    "\n",
    "# solve for true Lambda and estimated Lambda\n",
    "# by regressing theta_t onto y (with centering)\n",
    "m = x.shape[1]\n",
    "theta_sum = np.zeros([m,m])\n",
    "ytheta_sum = np.zeros(m)\n",
    "for i in range(T):\n",
    "  theta_centered = theta[i]-np.mean(theta[:T],axis=0)\n",
    "  theta_sum += np.outer(theta_centered,theta_centered)\n",
    "  ytheta_sum += theta_centered*(y[i]-np.mean(y[:T]))\n",
    "\n",
    "Lambda_hat = np.matmul(np.linalg.inv(theta_sum),ytheta_sum)\n",
    "print(Lambda_hat)\n",
    "\n",
    "Lambda = np.matmul(EWWT,theta_star)\n",
    "Lambda = np.array([Lambda[0,0],Lambda[0,1]]) # recast into vector\n",
    "\n",
    "# solve LP with Lambda and its estimate (Lambda_hat)\n",
    "c1 = [-Lambda[0], -Lambda[1]]\n",
    "c2 = [-Lambda_hat[0], -Lambda_hat[1]]\n",
    "A = [[1, 1]]\n",
    "b = [10]\n",
    "theta0_bounds = (-30, 30)\n",
    "theta1_bounds = (-3, 3)\n",
    "from scipy.optimize import linprog\n",
    "opt1 = linprog(c1, A_ub=A, b_ub=b, bounds=[theta0_bounds, theta1_bounds])\n",
    "opt2 = linprog(c2, A_ub=A, b_ub=b, bounds=[theta0_bounds, theta1_bounds])\n",
    "\n",
    "theta_ao = opt1.x\n",
    "theta_ao_hat = opt2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gU8DBhTCVXRV",
    "outputId": "f1850bd0-4b17-46e0-eb85-7f00bd780fea"
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Solve for true agent outcomes-maximizing theta_ao\n",
    "theta_ao = cp.Variable((len(Lambda)))\n",
    "\n",
    "# Create two constraints.\n",
    "constraints = [theta_ao[0] >= -30, theta_ao[0] <= 30,\n",
    "               theta_ao[1] >= -3, theta_ao[1] <= 3,\n",
    "               cp.norm(theta_ao) <= 10]\n",
    "\n",
    "# Form objective.\n",
    "obj = cp.Maximize(cp.matmul(theta_ao,Lambda))\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cp.Problem(obj, constraints)\n",
    "prob.solve()  # Returns the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbOMsQuWSCrr",
    "outputId": "5aa8eb54-846f-4ec6-ce34-8b749e564353"
   },
   "outputs": [],
   "source": [
    "# Solve for estimated agent outcomes-maximizing theta_ao_hat \n",
    "# (using estimate theta_star_hat\n",
    "theta_ao_hat = cp.Variable((len(Lambda_hat)))\n",
    "\n",
    "# Create two constraints.\n",
    "constraints = [theta_ao_hat[0] >= -30, theta_ao_hat[0] <= 30,\n",
    "               theta_ao_hat[1] >= -3, theta_ao_hat[1] <= 3,\n",
    "               cp.norm(theta_ao_hat) <= 10]\n",
    "\n",
    "# Form objective.\n",
    "obj = cp.Maximize(cp.matmul(theta_ao_hat,Lambda_hat))\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cp.Problem(obj, constraints)\n",
    "prob.solve()  # Returns the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iVeJh4RR80H",
    "outputId": "3d48a512-1c25-4ddc-e5cc-3b0da61bf6c9"
   },
   "outputs": [],
   "source": [
    "# agent outcomes for different theta\n",
    "theta_ols = ols(x,y,len(x)) # ols estimate of theta\n",
    "\n",
    "# initialize lists of x's\n",
    "x_star = np.zeros([num_applicants,z.shape[1]])\n",
    "x_ols = np.zeros([num_applicants,z.shape[1]])\n",
    "x_ao = np.zeros([num_applicants,z.shape[1]])\n",
    "x_ao_hat = np.zeros([num_applicants,z.shape[1]])\n",
    "\n",
    "for i in range(num_applicants):\n",
    "  x_star[i] = z[i] + np.matmul(WWT[i],theta_star)\n",
    "  x_ols[i] = z[i] + np.matmul(WWT[i],theta_ols)\n",
    "  x_ao[i] = z[i] + np.matmul(WWT[i],theta_ao.value)\n",
    "  x_ao_hat[i] = z[i] + np.matmul(WWT[i],theta_ao_hat.value)\n",
    "\n",
    "# clip x values\n",
    "def clip_x(x):\n",
    "  x[:,0] = np.clip(x[:,0],400,1600) # clip to [400, 1600]\n",
    "  x[:,1] = np.clip(x[:,1],0,4) # clip to [0, 4]\n",
    "  return x\n",
    "\n",
    "x_star = clip_x(x_star)\n",
    "x_ols = clip_x(x_ols)\n",
    "x_ao = clip_x(x_ao)\n",
    "x_ao_hat = clip_x(x_ao_hat)\n",
    "\n",
    "# true outcomes (college gpa)\n",
    "y_star = np.clip(np.matmul(x_star,theta_star) + g, 0, 4)\n",
    "y_ols = np.clip(np.matmul(x_ols,theta_star) + g, 0, 4)\n",
    "y_ao = np.clip(np.matmul(x_ao,theta_star) + g, 0, 4)\n",
    "y_ao_hat = np.clip(np.matmul(x_ao_hat,theta_star) + g, 0, 4)\n",
    "\n",
    "print(\"Mean reward playing theta*:\", np.mean(y_star))\n",
    "print(\"Mean reward playing theta_hat (OLS):\",np.mean(y_ols))\n",
    "print(\"Mean reward playing theta_ao (w/ true Lambda):\",np.mean(y_ao))\n",
    "print(\"Mean reward playing theta_ao_hat (w/ estimated Lambda_hat):\",np.mean(y_ao_hat))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "college_admissions_experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
